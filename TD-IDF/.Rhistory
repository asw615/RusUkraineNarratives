geom_violin() +
stat_summary(fun = mean, geom = "point", shape = 23, colour = "Black")
# Analysis of the following word
following_target <- df %>%
filter(order == 144)
# Using the shapiro test for the target word to see if data is normally distributed
round(stat.desc(cbind(rt_shapiro = following_target$rt), basic = FALSE, norm = TRUE), digits = 3)
# Since the data is not parametric (from the p-value being below 0.05), we will now try to transform it to see if that makes it parametric
following_target <- mutate(following_target, following_log = log(rt), following_sqrt = sqrt(rt), following_reciproc = 1/rt)
# Shapiro test again to see if the transformed data is normally distributed
round(stat.desc(cbind(Logarithmic = following_target$following_log, Squared = following_target$following_sqrt, Reciprocal = following_target$following_reciproc), basic = FALSE, norm = TRUE), digits = 3)
# From the looks of the p-value of the logarithmic data, it seems to be fairly normally distributed with a p-value of 0.974 and also fair skewness and kurtosis values
graph_log <-  hist_func(following_target, following_log, following_target$following_log, 0.075) +
ggtitle("Logarithmic transform") + # Titles
labs(x="Reaction times", y= "Density")
graph_log
# Levene's test
car::leveneTest(following_target$following_log, following_target$condition, center=mean)
# Now we're running an independent t-test on the two conditions to see how much variance they do not share
t.test(following_log ~ condition, data = following_target, var.equal = TRUE)
# After the t-test we don't have significance for the alternative null hypothesis.
# Let's visualize how the means differ
following_target %>%
ggplot(aes(x=condition, y=rt, fill=condition)) +
geom_violin() +
stat_summary(fun = mean, geom = "point", shape = 23, colour = "Black")
knitr::opts_chunk$set(echo = TRUE)
# installing packages
library(pacman)
p_load(tidyverse, pastecs, moments, rlang, WRS2, ggpubr)
# loading in the data
setwd(".")
df <- list.files(path = "/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/PsychoPy/Assignment/logfiles", pattern="*.csv", full.names = TRUE) %>% map_df(~ read_csv(.))
#anonymizing the data
df$id <- replace_na(df$id, "AAA")
random_IDs <- setNames(sample(n_distinct(df$id)), unique(df$id))
df <- df %>%
mutate(random_id = as.factor(
unlist(lapply(
df$id,
function(alias) random_IDs[[alias]]))
)
) %>%
select(-c(id))
## Preparing the data
# Adding a new column with the length of the words + making condition a factor
df <- mutate(df, word_length = nchar(word))
df <- df %>% rename(order = ...1)
df <- df %>% mutate(condition = as.factor(condition))
# Making all the data lower case, deleting commas and dots.
df <- mutate(df, "word"=tolower(df$word))
df$word <- sub(",", "", df$word)
df$word <- sub("\\.", "", df$word)
# Initial visualization of the data
df %>%
ggplot(aes(x=order, y=rt)) +
geom_text(label=df$word, color=df$random_id, alpha= 0.5)
# Remove outliers
df <- df %>%
filter(
rt > mean(rt)-3*sd(rt) & rt < mean(rt)+3*sd(rt)
)
# Loading in word frequencies from https://norvig.com/ngrams/count_1w.txt which has 333 million most frequent english
words_df <- read_table("count_1w.txt", col_names = c("word", "count"))
df <- left_join(df, words_df, by = "word")
# Making a data frame without the words "milk" and "cola" (which are the two conditions)
df_new <- df %>% filter(word != "cola", word != "milk")
# Making a function for plotting density histogram plots.
hist_func <- function(data, x, data_var, bin=0.1){
data %>%
ggplot(aes(x= {{x}})) + # Defining ggplot
geom_histogram(aes(y=..density..), fill="#69b3a2", color="#e9ecef", alpha=0.9, binwidth = {{bin}})+ # Making the histogram
theme_minimal() + # Adding themes
stat_function(fun = dnorm, args = list(mean = mean({{data_var}}), sd = sd({{data_var}}))) # The normal distributed line
}
# Figuring out if the data is normally distributed
# Using the shapiro test to see if the p-value is above 0.05
round(stat.desc(cbind(rt_shapiro = df_new$rt), basic = FALSE, norm = TRUE), digits = 3)
# Making a QQ-plot
qqnorm(df_new$rt, pch = 1, main = "Reaction time Q-Q plot")
qqline(df_new$rt, col = "steelblue", lwd = 2)
## From the p-value and both skewness and kurtosis we can see, that the data is not normally distributed. We will now try to transform the data to see if that makes it normal distributed
# Making a new data frame for the transformated data
df_trans <- mutate(df_new, rt_log = log(rt), rt_sqrt = sqrt(rt), rt_reciproc = 1/rt)
# Shapiro test
round(stat.desc(cbind(Logarithmic = df_trans$rt_log, Squared = df_trans$rt_sqrt, Reciprocal = df_trans$rt_reciproc), basic = FALSE, norm = TRUE), digits = 3)
# Plotting
hist_log <- hist_func(df_trans, rt_log, df_trans$rt_log) +
ggtitle("Histogram of logarithmic reaction times") + # Titles
labs(x="Logartihmic reaction time", y= "Density")
hist_sqrt <- hist_func(df_trans, rt_sqrt, df_trans$rt_sqrt) +
ggtitle("Histogram of square rooted reaction times") + # Titles
labs(x="Square rooted reaction times", y= "Density")
hist_reciproc <-  hist_func(df_trans, rt_reciproc, df_trans$rt_reciproc) +
ggtitle("Histogram of reciprocal reaction times") + # Titles
labs(x="Reciprocal reaction time", y= "Density")
ggarrange(hist_log, hist_sqrt, hist_reciproc)
# Since we can conclude that the reaction times are not normally distributed we will now move on to testing correlation of word length, ordinal word number, and word frequency. We use kendalls test for word length instead of spearmans because there are a lot of tied ranks.
cor_kendall_length <- cor.test(df_new$rt, df_new$word_length, method = 'kendall')
cor_spearman_order <- cor.test(df_new$rt, df_new$order, method = 'spearman', exact = F)
cor_spearman_freq <- cor.test(df_new$rt, df_new$count, method = 'spearman', exact = F)
cor_kendall_length
cor_spearman_order
cor_spearman_freq
# We see a correlation effect in the word order. However by the rho value, this effect is very low and it also makes logical sense why this is because the participants got more comfortable with the experiment while reading the text and some might have gotten impatient and scrolled through the text faster.
## Analysis of only the target word
target_word <- df %>%
group_by(condition) %>%
filter(order == 143)
# Using the shapiro test for the target word to see if data is normally distributed
round(stat.desc(cbind(rt_shapiro = target_word$rt), basic = FALSE, norm = TRUE), digits = 3)
# Since the data is not normally distributed (from the p-value being below 0.05), we will now try to transform it to see if that makes it normally distributed
target_trans <- mutate(target_word, target_log = log(rt), target_sqrt = sqrt(rt), target_reciproc = 1/rt)
# Using the shapiro test again to see whether or not, the data is now normally distributed
round(stat.desc(cbind(Log = target_trans$target_log, Sqrt = target_trans$target_sqrt, Reciproc = target_trans$target_reciproc), basic = FALSE, norm = TRUE), digits = 3)
# From the looks of the p-value (being p>0,05), kurtosis and skewness both the log transformed data seems to be normally distributed.
# Levenes test
car::leveneTest(target_trans$target_log, target_trans$condition, center=mean)
# We'll use the inverted transformation, since it has the highest p-value and also passes the Levene test with a p-value larger than 0.05 meaning that the variance is homogeneous. Let's visualize this.
hist_func(target_trans, target_log, target_trans$target_log, 0.075) +
ggtitle("Histogram of logarithmic reaction times") + # Titles
labs(x="Logarithmic reaction time", y= "Density")
# Now we're running an independent t-test on the two conditions to see how much variance they do not share
t.test(target_log ~ condition, data = target_trans, var.equal = TRUE)
# Let's visualize how the two means differ
target_trans %>%
ggplot(aes(x=condition, y=rt, fill=condition)) +
geom_violin() +
stat_summary(fun = mean, geom = "point", shape = 23, colour = "Black")
# Analysis of the following word
following_target <- df %>%
filter(order == 144)
# Using the shapiro test for the target word to see if data is normally distributed
round(stat.desc(cbind(rt_shapiro = following_target$rt), basic = FALSE, norm = TRUE), digits = 3)
# Since the data is not parametric (from the p-value being below 0.05), we will now try to transform it to see if that makes it parametric
following_target <- mutate(following_target, following_log = log(rt), following_sqrt = sqrt(rt), following_reciproc = 1/rt)
# Shapiro test again to see if the transformed data is normally distributed
round(stat.desc(cbind(Logarithmic = following_target$following_log, Squared = following_target$following_sqrt, Reciprocal = following_target$following_reciproc), basic = FALSE, norm = TRUE), digits = 3)
# From the looks of the p-value of the logarithmic data, it seems to be fairly normally distributed with a p-value of 0.974 and also fair skewness and kurtosis values
graph_log <-  hist_func(following_target, following_log, following_target$following_log, 0.075) +
ggtitle("Logarithmic transform") + # Titles
labs(x="Reaction times", y= "Density")
graph_log
# Levene's test
car::leveneTest(following_target$following_log, following_target$condition, center=mean)
# Now we're running an independent t-test on the two conditions to see how much variance they do not share
t.test(following_log ~ condition, data = following_target, var.equal = TRUE)
# After the t-test we don't have significance for the alternative null hypothesis.
# Let's visualize how the means differ
following_target %>%
ggplot(aes(x=condition, y=rt, fill=condition)) +
geom_violin() +
stat_summary(fun = mean, geom = "point", shape = 23, colour = "Black")
knitr::opts_chunk$set(echo = TRUE)
# loading in the data
setwd(".")
knitr::opts_chunk$set(echo = TRUE)
p_load(dplyr, janeaustenr, tidytext, tidyverse, forcats)
library(pacman)
p_load(dplyr, janeaustenr, tidytext, tidyverse, forcats)
p_load(dplyr, janeaustenr, tidytext, tidyverse, forcats)
setwd('.')
# Loading in data from the news articles and the telegram channels
news_russia <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_russia.csv")
news_ukraine <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_ukraine.csv")
tele <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/telegram-analysis-master/csv_data/df_no_na.csv")
#Removing unnecessary columns
tele_clean = subset(tele, select = c(id,date, message, views, forwards, edit_date, post_author, peer_id))
tele_msg = subset(tele, select = c(message))
news_ukraine_simple = subset(news_ukraine, select = c(date_publish, maintext))
news_russia_simple = subset(news_russia, select = c(date_publish, maintext))
# Rename the "date_publish" and "maintext" columns to "date" and "message"
colnames(news_russia_simple) <- c("date", "message")
colnames(news_ukraine_simple) <- c("date", "message")
colnames(tele_simple) <- c("date", "message")
# Making both newsarticles data and telegram look the same way
tele_simple = subset(tele_clean, select = c(date, english))
tele <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/telegram-analysis-master/csv_data/df_no_na.csv")
View(tele)
tele <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/telegram-analysis-master/csv_data/df_translate_final.csv")
#Removing unnecessary columns
tele_clean = subset(tele, select = c(id,date, message, views, forwards, edit_date, post_author, peer_id))
tele_msg = subset(tele, select = c(message))
# Making both newsarticles data and telegram look the same way
tele_simple = subset(tele_clean, select = c(date, english))
news_ukraine_simple = subset(news_ukraine, select = c(date_publish, maintext))
# Making both newsarticles data and telegram look the same way
tele_simple = subset(tele_clean, select = c(date, english))
View(tele)
tele <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/telegram-analysis-master/csv_data/df_translate_final.csv")
#Removing unnecessary columns
tele_clean = subset(tele, select = c(english, date))
# Making both newsarticles data and telegram look the same way
tele_simple = subset(tele_clean, select = c(date, english))
news_ukraine_simple = subset(news_ukraine, select = c(date_publish, maintext))
news_russia_simple = subset(news_russia, select = c(date_publish, maintext))
# Rename the "date_publish" and "maintext" columns to "date" and "message"
colnames(news_russia_simple) <- c("date", "message")
colnames(news_ukraine_simple) <- c("date", "message")
colnames(tele_simple) <- c("date", "message")
# Adding in their source in a new column
news_ukraine_simple <- mutate(news_russian_simple, source = "ukranian_newsarticles")
news_russia_simple <- mutate(news_russian_simple, source = "russian_newsarticles")
news_ukraine_simple = subset(news_ukraine, select = c(date_publish, maintext))
news_russia_simple = subset(news_russia, select = c(date_publish, maintext))
# Adding in their source in a new column
news_ukraine_simple <- mutate(news_ukraine_simple, source = "ukranian_newsarticles")
news_russia_simple <- mutate(news_russian_simple, source = "russian_newsarticles")
news_russia_simple <- mutate(news_russia_simple, source = "russian_newsarticles")
tele_simple <- mutate(tele_simple, source = "russian_telegram")
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(tele_simple, news_russia_simple, news_ukraine_simple)
# Adding in their source in a new column
news_ukraine_simple <- mutate(news_ukraine_simple, source = "ukranian_newsarticles")
news_russia_simple <- mutate(news_russia_simple, source = "russian_newsarticles")
tele_simple <- mutate(tele_simple, source = "russian_telegram")
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(tele_simple, news_russia_simple, news_ukraine_simple)
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(tele_simple, news_russia_simple, news_ukraine_simple)
View(news_ukraine_simple)
View(tele_simple)
View(news_ukraine_simple)
colnames(tele_simple) <- c("date", "message")
View(tele_simple)
colnames(tele_simple) <- c("date", "message", "source")
# Adding in their source in a new column
news_ukraine_simple <- mutate(news_ukraine_simple, source = "ukranian_newsarticles")
news_russia_simple <- mutate(news_russia_simple, source = "russian_newsarticles")
tele_simple <- mutate(tele_simple, source = "russian_telegram")
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(tele_simple, news_russia_simple, news_ukraine_simple)
View(tele_simple)
View(news_ukraine_simple)
# Rename the "date_publish" and "maintext" columns to "date" and "message"
colnames(news_russia_simple) <- c("date", "message")
View(news_russia_simple)
# Rename the "date_publish" and "maintext" columns to "date" and "message"
colnames(news_russia_simple) <- c("date", "message", "source")
colnames(news_ukraine_simple) <- c("date", "message", "source")
colnames(tele_simple) <- c("date", "message", "source")
View(news_russia_simple)
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(tele_simple, news_russia_simple, news_ukraine_simple)
# Loading in data from the news articles and the telegram channels
news_russia <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_russia.csv")
news_ukraine <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_ukraine.csv")
news_ukraine <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_ukraine.csv")
tele <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/telegram-analysis-master/csv_data/df_translate_final.csv")
news_ukraine <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_ukraine.csv")
tele <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/telegram-analysis-master/csv_data/df_translate_final.csv")
#Removing unnecessary columns
tele_clean = subset(tele, select = c(english, date))
# Making both newsarticles data and telegram look the same way
tele_simple = subset(tele_clean, select = c(date, english))
#Removing unnecessary columns
tele_clean = subset(tele, select = c(english, date))
# Making both newsarticles data and telegram look the same way
tele_simple = subset(tele_clean, select = c(date, english))
news_ukraine_simple = subset(news_ukraine, select = c(date_publish, maintext))
news_russia_simple = subset(news_russia, select = c(date_publish, maintext))
# Rename the "date_publish" and "maintext" columns to "date" and "message"
colnames(news_russia_simple) <- c("date", "message", "source")
colnames(news_ukraine_simple) <- c("date", "message", "source")
colnames(tele_simple) <- c("date", "message", "source")
# Adding in their source in a new column
news_ukraine_simple <- mutate(news_ukraine_simple, source = "ukranian_newsarticles")
news_russia_simple <- mutate(news_russia_simple, source = "russian_newsarticles")
tele_simple <- mutate(tele_simple, source = "russian_telegram")
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(tele_simple, news_russia_simple, news_ukraine_simple)
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(news_russia_simple, news_ukraine_simple)
View(df)
View(tele_simple)
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(news_russia_simple, news_ukraine_simple, tele_simple)
View(df)
if (any(!is.na(tele$english))) {
# If the "english" column has any values that are not NA,
# this code will be executed
print("The 'english' column has values other than NA.")
} else {
# If the "english" column has only NA values,
# this code will be executed
print("The 'english' column has only NA values.")
}
# unnesting/separating tokens in the maintext and counting words and authors
df_words <- df %>%
unnest_tokens(word, message) %>%
count(source, word, sort = TRUE)
# finding total words and number of times the word has occured
total_words <- df_words %>%
group_by(source) %>%
summarize(total = sum(n))
# Adding in the number number of times a word has occured and the total words
df_words <- left_join(df_words, total_words)
# Finding the frequency of words and sorting by rank
freq_by_rank <- df_words %>%
group_by(source) %>%
mutate(rank = row_number(),
`term frequency` = n/total) %>%
ungroup()
# Adding tf and idf values to the dataframe
tf_idf <- df_words %>%
bind_tf_idf(word, source, n)
tf_idf
# Looking at words with high tf-idf values
tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
# PLOTTING :)))))))
tf_idf %>%
group_by(source) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = source)) +
geom_col(show.legend = FALSE) +
facet_wrap(~source, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(news_russia_simple, news_ukraine_simple)
# unnesting/separating tokens in the maintext and counting words and authors
df_words <- df %>%
unnest_tokens(word, message) %>%
count(source, word, sort = TRUE)
# finding total words and number of times the word has occured
total_words <- df_words %>%
group_by(source) %>%
summarize(total = sum(n))
# Adding in the number number of times a word has occured and the total words
df_words <- left_join(df_words, total_words)
# Finding the frequency of words and sorting by rank
freq_by_rank <- df_words %>%
group_by(source) %>%
mutate(rank = row_number(),
`term frequency` = n/total) %>%
ungroup()
# Adding tf and idf values to the dataframe
tf_idf <- df_words %>%
bind_tf_idf(word, source, n)
tf_idf
# Looking at words with high tf-idf values
tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
# PLOTTING :)))))))
tf_idf %>%
group_by(source) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = source)) +
geom_col(show.legend = FALSE) +
facet_wrap(~source, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(news_russia_simple, news_ukraine_simple, tele_simple)
# unnesting/separating tokens in the maintext and counting words and authors
df_words <- df %>%
unnest_tokens(word, message) %>%
count(source, word, sort = TRUE)
# finding total words and number of times the word has occured
total_words <- df_words %>%
group_by(source) %>%
summarize(total = sum(n))
# Adding in the number number of times a word has occured and the total words
df_words <- left_join(df_words, total_words)
# Finding the frequency of words and sorting by rank
freq_by_rank <- df_words %>%
group_by(source) %>%
mutate(rank = row_number(),
`term frequency` = n/total) %>%
ungroup()
# Adding tf and idf values to the dataframe
tf_idf <- df_words %>%
bind_tf_idf(word, source, n)
tf_idf
# Looking at words with high tf-idf values
tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
# PLOTTING :)))))))
tf_idf %>%
group_by(source) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = source)) +
geom_col(show.legend = FALSE) +
facet_wrap(~source, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
View(tele_simple)
View(news_ukraine)
knitr::opts_chunk$set(echo = TRUE)
# Loading in data from the news articles and the telegram channels
news_russia <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_russia.csv")
news_ukraine <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_ukraine.csv")
# Making both newsarticles data and telegram look the same way
news_ukraine_simple = subset(news_ukraine, select = c(date_publish, title))
news_russia_simple = subset(news_russia, select = c(date_publish, title))
# Rename the "date_publish" and "maintext" columns to "date" and "message"
colnames(news_russia_simple) <- c("date", "title", "source")
colnames(news_ukraine_simple) <- c("date", "title", "source")
# Adding in their source in a new column
news_ukraine_simple <- mutate(news_ukraine_simple, source = "ukranian_newsarticles")
news_russia_simple <- mutate(news_russia_simple, source = "russian_newsarticles")
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(news_russia_simple, news_ukraine_simple, tele_simple)
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(news_russia_simple, news_ukraine_simple)
# unnesting/separating tokens in the maintext and counting words and authors
df_words <- df %>%
unnest_tokens(word, message) %>%
count(source, word, sort = TRUE)
# Loading in data from the news articles and the telegram channels
news_russia <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_russia.csv")
news_ukraine <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_ukraine.csv")
# Making both newsarticles data and telegram look the same way
news_ukraine_simple = subset(news_ukraine, select = c(date_publish, title))
news_russia_simple = subset(news_russia, select = c(date_publish, title))
# Rename the "date_publish" and "maintext" columns to "date" and "message"
colnames(news_russia_simple) <- c("date", "message", "source")
colnames(news_ukraine_simple) <- c("date", "message", "source")
# Adding in their source in a new column
news_ukraine_simple <- mutate(news_ukraine_simple, source = "ukranian_newsarticles")
news_russia_simple <- mutate(news_russia_simple, source = "russian_newsarticles")
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(news_russia_simple, news_ukraine_simple)
# unnesting/separating tokens in the maintext and counting words and authors
df_words <- df %>%
unnest_tokens(word, message) %>%
count(source, word, sort = TRUE)
# finding total words and number of times the word has occured
total_words <- df_words %>%
group_by(source) %>%
summarize(total = sum(n))
# Adding in the number number of times a word has occured and the total words
df_words <- left_join(df_words, total_words)
# Finding the frequency of words and sorting by rank
freq_by_rank <- df_words %>%
group_by(source) %>%
mutate(rank = row_number(),
`term frequency` = n/total) %>%
ungroup()
# Adding tf and idf values to the dataframe
tf_idf <- df_words %>%
bind_tf_idf(word, source, n)
tf_idf
# Looking at words with high tf-idf values
tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
# PLOTTING :)))))))
tf_idf %>%
group_by(source) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = source)) +
geom_col(show.legend = FALSE) +
facet_wrap(~source, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
knitr::opts_chunk$set(echo = TRUE)
# Loading in data from the news articles and the telegram channels
news_russia <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_russia.csv")
news_ukraine <- read_csv("/Users/sorenmeiner/Library/CloudStorage/OneDrive-Aarhusuniversitet/CogComm exam/webscraper/csv_data/df_ukraine.csv")
# Making both newsarticles data and telegram look the same way
news_ukraine_simple = subset(news_ukraine, select = c(date_publish, description))
news_russia_simple = subset(news_russia, select = c(date_publish, description))
# Rename the "date_publish" and "maintext" columns to "date" and "message"
colnames(news_russia_simple) <- c("date", "message", "source")
colnames(news_ukraine_simple) <- c("date", "message", "source")
# Adding in their source in a new column
news_ukraine_simple <- mutate(news_ukraine_simple, source = "ukranian_newsarticles")
news_russia_simple <- mutate(news_russia_simple, source = "russian_newsarticles")
# Adding both newsarticles and telegram channels to the same dataset
df = rbind(news_russia_simple, news_ukraine_simple)
# unnesting/separating tokens in the maintext and counting words and authors
df_words <- df %>%
unnest_tokens(word, message) %>%
count(source, word, sort = TRUE)
# finding total words and number of times the word has occured
total_words <- df_words %>%
group_by(source) %>%
summarize(total = sum(n))
# Adding in the number number of times a word has occured and the total words
df_words <- left_join(df_words, total_words)
# Finding the frequency of words and sorting by rank
freq_by_rank <- df_words %>%
group_by(source) %>%
mutate(rank = row_number(),
`term frequency` = n/total) %>%
ungroup()
# Adding tf and idf values to the dataframe
tf_idf <- df_words %>%
bind_tf_idf(word, source, n)
tf_idf
# Looking at words with high tf-idf values
tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
# PLOTTING :)))))))
tf_idf %>%
group_by(source) %>%
slice_max(tf_idf, n = 15) %>%
ungroup() %>%
ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = source)) +
geom_col(show.legend = FALSE) +
facet_wrap(~source, ncol = 2, scales = "free") +
labs(x = "tf-idf", y = NULL)
knitr::opts_chunk$set(echo = TRUE)
p_load(dplyr, janeaustenr, tidytext, tidyverse, forcats, tm)
library(pacman)
p_load(dplyr, janeaustenr, tidytext, tidyverse, forcats, tm)
